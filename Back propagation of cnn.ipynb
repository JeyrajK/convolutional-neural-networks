{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation in Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets assume the function f is a convolution between Input X and a Filter F. Input X is a 3x3 matrix and Filter F is a 2x2 matrix, as shown below:\n",
    "\n",
    "![](https://miro.medium.com/max/692/1*VNr0GiFEwjmwj2v9YmPn5Q.png)\n",
    "\n",
    "Convolution between Input X and Filter F, gives us an output O. This can be represented as:\n",
    "\n",
    "![](https://miro.medium.com/max/1392/1*Q2GGz43E-o5FEtaDXuw8tA.png)\n",
    "\n",
    "![](https://miro.medium.com/max/1000/1*K7dINARev0NUB-HWp9mbwA.gif)\n",
    "\n",
    "This gives us the forward pass! Let’s get to the Backward pass. As mentioned earlier, we get the loss gradient with respect to the Output O from the next layer as ∂L/∂O, during Backward pass. And combining with our previous knowledge using Chain rule and Backpropagation we get:\n",
    "\n",
    "![](https://miro.medium.com/max/800/1*w8VkZ50foXWTmoXDDnr8tg.png)\n",
    "\n",
    "As seen above, we can find the local gradients ∂O/∂X and ∂O/∂F with respect to Output O. And with loss gradient from previous layers — ∂L/∂O and using chain rule, we can calculate ∂L/∂X and ∂L/∂F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let’s find the gradients for X and F — ∂L/∂X and ∂L/∂F\n",
    "\n",
    "### Finding ∂L/∂F\n",
    "\n",
    "This has two steps as we have done earlier.\n",
    "Find the local gradient ∂O/∂F\n",
    "Find ∂L/∂F using chain rule\n",
    "\n",
    "![](https://miro.medium.com/max/418/1*2nAF7_I4J7xLtpS8m_mXeA.png)\n",
    "\n",
    "If we look closely this above equation can be written in form of our convolution operation.\n",
    "\n",
    "![](https://miro.medium.com/max/907/1*auj7ULC2kRCa99_6u1QSNA.jpeg)\n",
    "\n",
    "### Finding ∂L/∂X\n",
    "\n",
    "Similarly we can find the gradients of the input matrix ‘X’ with respect to the error ‘E’.\n",
    "\n",
    "![](https://miro.medium.com/max/414/1*ndW3KqLjW9ht_8ZFjQOHDw.png)\n",
    "\n",
    "Now, the above computation can be obtained by a different type of convolution operation known as full convolution. In order to obtain the gradients of the input matrix we need to rotate the filter by 180 degree and calculate the full convolution of the rotated filter by the gradients of the output with respect to error, As represented in the image below.\n",
    "\n",
    "![](https://miro.medium.com/max/1189/1*YX5CVe6W7sOpKqJ8b2dVhg.jpeg)\n",
    "\n",
    "The full convolution can be visualized as carrying out the procedure as represented in the figure below.\n",
    "\n",
    "![](https://miro.medium.com/max/510/1*OiCxYNaVWOhOu36AXZoPgQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of forward and backward pass of a convolution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(X, W):\n",
    "    '''\n",
    "    The forward computation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    X -- output activations of the previous layer, numpy array of shape (n_H_prev, n_W_prev) assuming input channels = 1\n",
    "    W -- Weights, numpy array of size (f, f) assuming number of filters = 1\n",
    "    \n",
    "    Returns:\n",
    "    H -- conv output, numpy array of size (n_H, n_W)\n",
    "    cache -- cache of values needed for conv_backward() function\n",
    "    '''\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    (n_H_prev, n_W_prev) = X.shape\n",
    "    \n",
    "    # Retrieving dimensions from W's shape\n",
    "    (f, f) = W.shape\n",
    "    \n",
    "    # Compute the output dimensions assuming no padding and stride = 1\n",
    "    n_H = n_H_prev - f + 1\n",
    "    n_W = n_W_prev - f + 1\n",
    "    \n",
    "    # Initialize the output H with zeros\n",
    "    H = np.zeros((n_H, n_W))\n",
    "    \n",
    "    # Looping over vertical(h) and horizontal(w) axis of output volume\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            x_slice = X[h:h+f, w:w+f]\n",
    "            H[h,w] = np.sum(x_slice * W)\n",
    "            \n",
    "    # Saving information in 'cache' for backprop\n",
    "    cache = (X, W)\n",
    "    \n",
    "    return H, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dH, cache):\n",
    "    '''\n",
    "    The backward computation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dH -- gradient of the cost with respect to output of the conv layer (H), numpy array of shape (n_H, n_W) assuming channels = 1\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dX -- gradient of the cost with respect to input of the conv layer (X), numpy array of shape (n_H_prev, n_W_prev) assuming channels = 1\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f,f) assuming single filter\n",
    "    '''\n",
    "    \n",
    "    # Retrieving information from the \"cache\"\n",
    "    (X, W) = cache\n",
    "    \n",
    "    # Retrieving dimensions from X's shape\n",
    "    (n_H_prev, n_W_prev) = X.shape\n",
    "    \n",
    "    # Retrieving dimensions from W's shape\n",
    "    (f, f) = W.shape\n",
    "    \n",
    "    # Retrieving dimensions from dH's shape\n",
    "    (n_H, n_W) = dH.shape\n",
    "    \n",
    "    # Initializing dX, dW with the correct shapes\n",
    "    dX = np.zeros(X.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    # Looping over vertical(h) and horizontal(w) axis of the output\n",
    "    for h in range(n_H):\n",
    "        for w in range(n_W):\n",
    "            dW += X[h:h+f, w:w+f] * dH(h,w) # or dW[h,w] =  np.sum(X[h:h+f, w:w+f] * dH)\n",
    "            dX[h:h+f, w:w+f] += W * dH(h,w) \n",
    "    \n",
    "    return dX, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
