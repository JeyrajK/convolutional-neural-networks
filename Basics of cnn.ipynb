{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.\n",
    "\n",
    "The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer — The Kernel\n",
    "\n",
    "Image Dimensions = 5 (Height) x 5 (Breadth) x 1 (Number of channels, eg. RGB)\n",
    "\n",
    "![](https://miro.medium.com/max/526/1*GcI7G-JLAQiEoCON7xFbhg.gif)\n",
    "\n",
    "In the above demonstration, the green section resembles our 5x5x1 input image, I. The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the Kernel/Filter, K, represented in the color yellow. We have selected K as a 3x3x1 matrix.\n",
    "\n",
    "The Kernel shifts 9 times because of Stride Length = 1 (Non-Strided), every time performing a matrix multiplication operation between K and the portion P of the image over which the kernel is hovering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter moves to the right with a certain Stride Value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1280/1*ciDgQEjViWLnCbmX-EeSrA.gif)\n",
    "\n",
    "\n",
    "In the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Matrix Multiplication is performed between Kn and In stack ([K1, I1]; [K2, I2]; [K3, I3]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output.\n",
    "\n",
    "The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.\n",
    "\n",
    "![](https://miro.medium.com/max/395/1*1VJDP6qDY9-ExTuQVEOlVg.gif)\n",
    "\n",
    "There are two types of results to the operation — one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying Valid Padding in case of the former, or Same Padding in the case of the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we augment the 5x5x1 image into a 6x6x1 image and then apply the 3x3x1 kernel over it, we find that the convolved matrix turns out to be of dimensions 5x5x1. Hence the name — Same Padding.\n",
    "On the other hand, if we perform the same operation without padding, we are presented with a matrix which has dimensions of the Kernel (3x3x1) itself — Valid Padding.\n",
    "\n",
    "![](https://miro.medium.com/max/395/1*nYf_cUIHFEWU1JXGwnz-Ig.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pooling Layer\n",
    "\n",
    "![](https://miro.medium.com/max/396/1*uoWYsCV5vBU8SHFPAPao-w.gif)\n",
    "\n",
    "Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n",
    "\n",
    "There are two types of Pooling: Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel.\n",
    "Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. On the other hand, Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling.\n",
    "\n",
    "![](https://miro.medium.com/max/596/1*KQIEqhxzICU7thjaQBfPBQ.png)\n",
    "\n",
    "The Convolutional Layer and the Pooling Layer, together form the i-th layer of a Convolutional Neural Network. Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power.\n",
    "\n",
    "After going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilated Convolutions\n",
    "\n",
    "Dilated convolutions introduce another parameter to convolutional layers called the dilation rate. This defines a spacing between the values in a kernel. A 3x3 kernel with a dilation rate of 2 will have the same field of view as a 5x5 kernel, while only using 9 parameters. Imagine taking a 5x5 kernel and deleting every second column and row.\n",
    "\n",
    "![](https://miro.medium.com/max/395/1*SVkgHoFoiMZkjy54zM_SUw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification — Fully Connected Layer (FC Layer)\n",
    "\n",
    "![](https://miro.medium.com/max/802/1*kToStLowjokojIQ7pY2ynQ.jpeg)\n",
    "\n",
    "Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.\n",
    "Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution layers\n",
    "#### Conv1d\n",
    "\n",
    "Applies a 1D convolution over an input signal composed of several input planes.\n",
    "\n",
    "Shape:\n",
    "Input: $(N, C_{in}, L_{in})$\n",
    "Output: $(N, C_{out}, L_{out})$ where\n",
    "\n",
    "![](Pictures/conv1d.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "inp = torch.randn(20, 16, 50)\n",
    "output = m(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv2d\n",
    "\n",
    "Applies a 2D convolution over an input signal composed of several input planes.\n",
    "\n",
    "Shape:\n",
    "Input: $(N, C_{in}, H_{in}, W_{in})$\n",
    "\n",
    "Output: $(N, C_{out}, H_{out}, W_{out})$ where\n",
    "\n",
    "![](Pictures/conv2d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 24, 49])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.Conv2d(16, 33, 3, stride=2)\n",
    "inp = torch.randn(20, 16, 50, 100)\n",
    "output = m(inp)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 28, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "inp = torch.randn(20, 16, 50, 100)\n",
    "output = m(inp)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 26, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-square kernels and unequal stride and with padding and dilation\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "inp = torch.randn(20, 16, 50, 100)\n",
    "output = m(inp)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv3d\n",
    "\n",
    "Applies a 3D convolution over an input signal composed of several input planes.\n",
    "\n",
    "Shape:\n",
    "Input: $(N, C_{in}, D_{in}, H_{in}, W_{in})$\n",
    "\n",
    "Output: $(N, C_{out}, D_{out}, H_{out}, W_{out})$ where\n",
    "\n",
    "![](Pictures/conv3d.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 4, 24, 49])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.Conv3d(16, 33, 3, stride=2)\n",
    "inp = torch.randn(20, 16, 10, 50, 100)\n",
    "output = m(inp)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 8, 50, 99])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n",
    "inp = torch.randn(20, 16, 10, 50, 100)\n",
    "output = m(inp)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 15, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(15, 30, kernel_size=3, stride=2)\n",
    "        self.fc1 = nn.Linear(1080, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv1(kernel=3, filters=15) 28x28x1 -> 26x26x15\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # conv2(kernel=3, filters=20) 26x26x15 -> 13x13x30\n",
    "        # max_pool(kernel=2) 13x13x30 -> 6x6x30\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2, stride=2))\n",
    "\n",
    "        # flatten 6x6x30 = 1080\n",
    "        x = x.view(-1, 1080)\n",
    "\n",
    "        # 1080 -> 100\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # 100 -> 10\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # transform to logits\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTClass(\n",
       "  (conv1): Conv2d(1, 15, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(15, 30, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=1080, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_num_correct(pred,Y):\n",
    "    return pred.argmax(dim=1).eq(Y).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 705 loss: 1.4361859418096996\n",
      "epoch 1 total_correct: 1193 loss: 0.3890482286612193\n",
      "epoch 2 total_correct: 1230 loss: 0.26689529028676806\n",
      "epoch 3 total_correct: 1271 loss: 0.17319698809158235\n",
      "epoch 4 total_correct: 1265 loss: 0.18897668538349016\n",
      "epoch 5 total_correct: 1281 loss: 0.14157835873109953\n",
      "epoch 6 total_correct: 1294 loss: 0.12438674350934369\n",
      "epoch 7 total_correct: 1288 loss: 0.1539668036358697\n",
      "epoch 8 total_correct: 1286 loss: 0.13928663588705517\n",
      "epoch 9 total_correct: 1298 loss: 0.1250829054486184\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for i,batch in enumerate(trainloader): \n",
    "        \n",
    "        X,Y = batch\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(pred,Y)\n",
    "        \n",
    "        if i>=20:\n",
    "            break\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss/(i+1)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
